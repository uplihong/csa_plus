# @package _global_
experiment_name: limit_longest_random_cut
experiment_output_dir: outputs/${experiment_name}/${now:%Y-%m-%d}/${now:%H-%M}


deepspeed_config_yaml:
  train_batch_size: 96
  micro_batch_per_gpu: 24
  tensorboard:
    output_path: ${experiment_output_dir}


train:
  seed: 123456
  min_max_audio_second: [1.0, 6.0]
  # 训练好的CSA模型，在此基础上进一步cut audio训练
  pretrained_model_checkpoint: /data/huanglh/code/ContrastiveAT/output/wav2vec2base_extractedtextfeature_freezefeatureencoder_32batchsize_4gpu_amp/ckpt_epoch_8.pth
  tensorboard_dir: ${experiment_output_dir}
  max_step_iterations: ${deepspeed_config_yaml.scheduler.params.total_num_steps}

  checkpoint_every_steps: 1000
  validation_every_steps: 5000
  log_every_steps: 4

  evaluation:
    eval_batch_size: 8
    sequential: false

  data:
    num_workers: 0
    pin_memory: true



hydra:
  run:
    # dir: outputs/${experiment_name}/${now:%Y-%m-%d}/${now:%H-%M}
    dir: ${experiment_output_dir}
  verbose: false


# deepspeed --no_local_rank --num_gpus 4 pretrain_csa_ds_limit_longest.py +experiment=limit_longest
# deepspeed --no_local_rank --include localhost:0,1,2,3  pretrain_csa_ds_limit_longest.py +experiment=limit_longest