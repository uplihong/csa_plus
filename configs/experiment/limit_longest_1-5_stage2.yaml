# @package _global_
experiment_name: limit_longest_random_cut_1-5_stage2_batch128_no_offload  # 20000 steps收敛时性能较优
# experiment_name: nolimit_nocut_stage2_batch128_no_offload  
experiment_output_dir: outputs/${experiment_name}/${now:%Y-%m-%d}/${now:%H-%M}


deepspeed_config_yaml:
#  train_batch_size: 96
#  train_batch_size: 128
  gradient_accumulation_steps: 1
  train_micro_batch_size_per_gpu: 128

  zero_optimization:
    offload_optimizer: null

  tensorboard:
    output_path: ${experiment_output_dir}
    job_name: ${experiment_name}


train:
  seed: 123456
#  min_max_audio_second: [1.0, 6.0]
  min_max_audio_second: [1.0, 5.0]
  # 训练好的CSA模型，在此基础上进一步cut audio训练
  pretrained_model_checkpoint: /data/huanglh/code/ContrastiveAT/output/wav2vec2base_extractedtextfeature_freezefeatureencoder_32batchsize_4gpu_amp/ckpt_epoch_8.pth
  tensorboard_dir: ${experiment_output_dir}
  max_step_iterations: ${deepspeed_config_yaml.scheduler.params.total_num_steps}

  checkpoint_every_steps: 1000
  validation_every_steps: 5000
  log_every_steps: 4

  evaluation:
    # eval_batch_size: 32  # in csa this is set to 32
    eval_batch_size: 128  # in csa this is set to 32
    sequential: false

  data:
    num_workers: 2
    pin_memory: true



hydra:
  run:
    # dir: outputs/${experiment_name}/${now:%Y-%m-%d}/${now:%H-%M}
    dir: ${experiment_output_dir}
  verbose: false

# 24.12.10
# deepspeed --no_local_rank --num_gpus 4 pretrain_csa_ds_limit_longest.py +experiment=limit_longest_stage2

# 24.12.14
# deepspeed --no_local_rank --include localhost:1,3  pretrain_csa_ds_limit_longest.py +experiment=limit_longest_1-5_stage2

# 26.2.3
# NCCL_P2P_DISABLE=1 deepspeed \
#   --include 'localhost:0,4' train.py \
#   '++train.max_step_iterations=100000' \
#   '++train.log_every_steps=1' \
#   '++train.validation_every_steps=500' \
#   '++train.checkpoint_every_steps=500' \
#   'deepspeed_config_yaml.train_micro_batch_size_per_gpu=128' \
#   '+experiment=limit_longest_1-5_stage2'