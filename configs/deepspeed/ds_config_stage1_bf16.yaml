# deepspeed_config stage 1 (BF16, tuned for RTX 4090 multi-GPU)
train_micro_batch_size_per_gpu: 128
gradient_accumulation_steps: 1
steps_per_print: 20

optimizer:
  type: Adam
  params:
    lr: 0.0001
    betas: [0.8, 0.999]
    eps: 1e-8
    weight_decay: 3e-7

scheduler:
  type: WarmupCosineLR
  params:
    total_num_steps: 100000
    warmup_min_ratio: 0.0
    warmup_num_steps: 10000
    cos_min_ratio: 0.0001

gradient_clipping: 1.0
prescale_gradients: false
wall_clock_breakdown: false

bf16:
  enabled: true

fp16:
  enabled: false

zero_optimization:
  stage: 1
  allgather_partitions: true
  allgather_bucket_size: 200000000
  overlap_comm: true
  reduce_scatter: true
  reduce_bucket_size: 200000000
  contiguous_gradients: true

tensorboard:
  enabled: true
  output_path: ???
