# deepspeed_config stage 2 (BF16)
train_micro_batch_size_per_gpu: 256
gradient_accumulation_steps: 1
steps_per_print: 4
optimizer:
  type: Adam
  params:
    lr: 0.0001
    betas: [0.8, 0.999]
    eps: 1e-8
    weight_decay: 3e-7
scheduler:
  type: WarmupCosineLR
  params:
    total_num_steps: 1e5
    warmup_min_ratio: 0
    warmup_num_steps: 1e4
    cos_min_ratio: 1e-4
gradient_clipping: 1.0
prescale_gradients: False
bf16:
  enabled: true
fp16:
  enabled: false
wall_clock_breakdown: False
zero_optimization:
  stage: 2
  contiguous_gradients: true
  reduce_scatter: true
  reduce_bucket_size: 5e8
#  reduce_bucket_size: 2e8
#  reduce_bucket_size: 1e8
  allgather_partitions: true
  allgather_bucket_size: 5e8
#  allgather_bucket_size: 2e8
#  allgather_bucket_size: 1e8
  overlap_comm: true

#  offload_param:
#    device: cpu

  offload_optimizer:
    device: cpu
    pin_memory: true  # optimized for efficient CPU-GPU transfer during offload

tensorboard:
  enabled: true
  output_path: ???

#dataloader_drop_last: True  # drop last batch if it's not full, this is important for distributed training
