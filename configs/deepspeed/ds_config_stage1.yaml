train_batch_size: 256
gradient_accumulation_steps: 1
steps_per_print: 20

optimizer:
  type: Adam
  params:
    lr: 0.0001
    betas:
      - 0.8
      - 0.999
    eps: 1e-8
    weight_decay: 3e-7

scheduler:
  type: WarmupCosineLR
  params:
    total_num_steps: 1e5
    warmup_min_ratio: 0
    warmup_num_steps: 1e4
    cos_min_ratio: 1e-4

zero_optimization:
  stage: 1
zero_allow_untested_optimizer: true

fp16:
  enabled: true
  auto_cast: False
  loss_scale: 0
  initial_scale_power: 16
  loss_scale_window: 1000
  hysteresis: 2
  consecutive_hysteresis: False
  min_loss_scale: 1

gradient_clipping: 1
prescale_gradients: false
wall_clock_breakdown: false

tensorboard:
  enabled: true
  output_path: ???