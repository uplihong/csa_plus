# deepspeed_config stage 0
train_micro_batch_size_per_gpu: 256
gradient_accumulation_steps: 1
steps_per_print: 2000
optimizer:
  type: FusedAdam
  params:
    lr: 0.001
    betas: [0.8, 0.999]
    eps: 1e-8
    weight_decay: 3e-7
scheduler:
  type: WarmupLR
  params:
    warmup_min_lr: 0
    warmup_max_lr: 0.001
    warmup_num_steps: 1000
gradient_clipping: 1.0
prescale_gradients: False
fp16:
  enabled: True
  auto_cast: False
  loss_scale: 0
  initial_scale_power: 16
  loss_scale_window: 1000
  hysteresis: 2
  consecutive_hysteresis: False
  min_loss_scale: 1
wall_clock_breakdown: False
zero_optimization:
  stage: 0
  allgather_partitions: True
  reduce_scatter: True
  allgather_bucket_size: 50000000
  reduce_bucket_size: 50000000
  overlap_comm: True
  contiguous_gradients: True
  offload_optimizer: False
tensorboard:
  enabled: true
  output_path: ???
